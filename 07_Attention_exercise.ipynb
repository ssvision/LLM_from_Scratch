{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Exercise as prescribed by the video https://www.youtube.com/watch?v=wdlp4Sg01Mg&list=WL&index=1&t=120s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 2.0.0+cu118\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Solve This Challenge to Understand Self-Attention! (Transformers Explained)\n",
    "\n",
    "This file provides a hands-on challenge to implement a basic self-attention\n",
    "mechanism step-by-step using PyTorch.\n",
    "\n",
    "Self-attention allows a model to weigh the importance of different words (or tokens)\n",
    "in an input sequence when processing a specific word. It looks at other words\n",
    "in the sequence to get a better representation of the current word.\n",
    "\n",
    "Follow the comments and fill in the missing code sections marked with # FILL THIS IN.\n",
    "Run the script after filling each part to check your progress! Good luck!\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "print(f\"Using PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setup ---\n",
      "Batch Size (N): 1\n",
      "Sequence Length (L): 4\n",
      "Embedding Dimension (E = D_model): 8\n",
      "Key/Query Dimension (D_k): 8\n",
      "Value Dimension (D_v): 8\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "# Let's define some dimensions for our example.\n",
    "# We'll use a small sequence and embedding size for clarity.\n",
    "\n",
    "batch_size = 1  # How many sequences we process at once (keep at 1 for simplicity)\n",
    "seq_len = 4     # The length of our input sequence (e.g., 4 words)\n",
    "embed_dim = 8  # The dimension of each word embedding vector\n",
    "\n",
    "# For simplicity in this basic example, we'll make the dimensions for\n",
    "# Query, Key, and Value vectors the same as the embedding dimension.\n",
    "# In full transformer models, these can differ (especially with multi-head attention).\n",
    "d_k = embed_dim # Dimension of Key and Query vectors\n",
    "d_v = embed_dim # Dimension of Value vectors\n",
    "\n",
    "print(f\"\\n--- Setup ---\")\n",
    "print(f\"Batch Size (N): {batch_size}\")\n",
    "print(f\"Sequence Length (L): {seq_len}\")\n",
    "print(f\"Embedding Dimension (E = D_model): {embed_dim}\")\n",
    "print(f\"Key/Query Dimension (D_k): {d_k}\")\n",
    "print(f\"Value Dimension (D_v): {d_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Input ---\n",
      "Input tensor 'x' shape: torch.Size([1, 4, 8])\n",
      "Input tensor\n",
      " : tensor([[[ 0.8626, -0.8381, -1.0351, -0.3404,  0.1873,  0.1551, -0.3786,\n",
      "          -0.4157],\n",
      "         [-0.4404, -0.9564,  1.3785,  0.6161,  0.5635,  2.6217,  1.0450,\n",
      "          -2.1747],\n",
      "         [-0.5136,  0.4162, -0.6192,  1.4304, -1.9323, -0.5244, -1.2391,\n",
      "          -0.1437],\n",
      "         [ 1.6148, -0.1665, -1.5250,  1.1092,  1.9637, -0.5266, -0.0132,\n",
      "          -0.4935]]])\n"
     ]
    }
   ],
   "source": [
    "# --- Input Data ---\n",
    "# Let's create some random input data representing word embeddings.\n",
    "# Shape: (batch_size, seq_len, embed_dim) or (N, L, E)\n",
    "print(\"\\n--- Input ---\")\n",
    "x = torch.randn(batch_size, seq_len, embed_dim)\n",
    "print(f\"Input tensor 'x' shape: {x.shape}\")\n",
    "print(f\"Input tensor\\n : {x}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Linear Projections for Q, K, V ---\n",
    "# In self-attention, we project the input embeddings into three different spaces:\n",
    "# Query (Q): Represents the current word asking for information.\n",
    "# Key (K): Represents all words' potential relevance (as keys to be queried).\n",
    "# Value (V): Represents the actual content/meaning of all words.\n",
    "# We use learnable linear layers (weight matrices) for these projections.\n",
    "\n",
    "W_q = nn.Linear(embed_dim, d_k, bias=False) # Query weight matrix\n",
    "W_k = nn.Linear(embed_dim, d_k, bias=False) # Key weight matrix\n",
    "W_v = nn.Linear(embed_dim, d_v, bias=False) # Value weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Challenge 1: Calculate Q, K, V ---\n",
      "Q shape: torch.Size([1, 4, 8]) - Correct!\n",
      "K shape: torch.Size([1, 4, 8]) - Correct!\n",
      "V shape: torch.Size([1, 4, 8]) - Correct!\n",
      "Challenge 1 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Challenge 1: Calculate Q, K, V ---\")\n",
    "# ---- CHALLENGE 1 START ----\n",
    "# Your task: Apply the linear layers (W_q, W_k, W_v) to the input 'x'\n",
    "# to get the Query, Key, and Value matrices.\n",
    "# The input 'x' has shape (N, L, E).\n",
    "# Q, K should have shape (N, L, D_k).\n",
    "# V should have shape (N, L, D_v).\n",
    "\n",
    "# FILL THIS IN: Calculate Q by passing x through W_q\n",
    "Q = W_q(x) # Replace None with your calculation\n",
    "# FILL THIS IN: Calculate K by passing x through W_k\n",
    "K = W_k(x) # Replace None with your calculation\n",
    "# FILL THIS IN: Calculate V by passing x through W_v\n",
    "V = W_v(x) # Replace None with your calculation\n",
    "\n",
    "# ---- CHALLENGE 1 END ----\n",
    "\n",
    "# Let's check if Q, K, V were calculated\n",
    "assert Q is not None, \"Challenge 1 incomplete: Q is not calculated.\"\n",
    "assert K is not None, \"Challenge 1 incomplete: K is not calculated.\"\n",
    "assert V is not None, \"Challenge 1 incomplete: V is not calculated.\"\n",
    "# Check the shapes (should match the comments above)\n",
    "expected_q_k_shape = (batch_size, seq_len, d_k)\n",
    "expected_v_shape = (batch_size, seq_len, d_v)\n",
    "assert Q.shape == expected_q_k_shape, f\"Q shape is {Q.shape}, expected {expected_q_k_shape}\"\n",
    "assert K.shape == expected_q_k_shape, f\"K shape is {K.shape}, expected {expected_q_k_shape}\"\n",
    "assert V.shape == expected_v_shape, f\"V shape is {V.shape}, expected {expected_v_shape}\"\n",
    "print(f\"Q shape: {Q.shape} - Correct!\")\n",
    "print(f\"K shape: {K.shape} - Correct!\")\n",
    "print(f\"V shape: {V.shape} - Correct!\")\n",
    "print(\"Challenge 1 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Challenge 2: Calculate Raw Attention Scores (QK^T) ---\n",
      "Raw Attention Scores shape: torch.Size([1, 4, 4]) - Correct!\n",
      "Challenge 2 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Calculate Raw Attention Scores ---\n",
    "# The core idea: How much should each word (represented by Q) pay attention\n",
    "# to every other word (represented by K)?\n",
    "# We calculate this using the dot product between each Query vector and all Key vectors.\n",
    "# Formula part: QK^T\n",
    "\n",
    "print(\"\\n--- Challenge 2: Calculate Raw Attention Scores (QK^T) ---\")\n",
    "# ---- CHALLENGE 2 START ----\n",
    "# Your task: Calculate the raw attention scores by performing a matrix multiplication\n",
    "# between the Query (Q) and the transpose of the Key (K).\n",
    "# Q shape: (N, L, D_k)\n",
    "# K shape: (N, L, D_k) -> K transposed shape: (N, D_k, L)\n",
    "# The result should have shape (N, L, L), representing the scores\n",
    "# for each query position attending to each key position.\n",
    "# Hint: Use torch.matmul() and transpose K correctly (the last two dimensions).\n",
    "\n",
    "# FILL THIS IN: Calculate Q * K^T\n",
    "attention_scores_raw = Q @ K.transpose(1,2) # Replace None with your calculation\n",
    "\n",
    "# ---- CHALLENGE 2 END ----\n",
    "\n",
    "assert attention_scores_raw is not None, \"Challenge 2 incomplete: attention_scores_raw is not calculated.\"\n",
    "expected_scores_shape = (batch_size, seq_len, seq_len)\n",
    "assert attention_scores_raw.shape == expected_scores_shape, f\"Raw Attention Scores shape is {attention_scores_raw.shape}, expected {expected_scores_shape}\"\n",
    "print(f\"Raw Attention Scores shape: {attention_scores_raw.shape} - Correct!\")\n",
    "# print(f\"Raw Scores (example):\\n{attention_scores_raw.detach()}\") # Optional: uncomment to view scores\n",
    "print(\"Challenge 2 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Challenge 3: Scale the Scores ---\n",
      "Scale factor (sqrt(d_k)): 2.83\n",
      "Scaled Attention Scores shape: torch.Size([1, 4, 4]) - Correct!\n",
      "Challenge 3 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Scale the Scores ---\n",
    "# To stabilize gradients during training, the scores are scaled down.\n",
    "# We divide by the square root of the Key/Query dimension (d_k).\n",
    "# Formula part: QK^T / sqrt(d_k)\n",
    "\n",
    "print(\"\\n--- Challenge 3: Scale the Scores ---\")\n",
    "# ---- CHALLENGE 3 START ----\n",
    "# Your task: Scale the `attention_scores_raw` by dividing them by the\n",
    "# square root of d_k.\n",
    "# Hint: Use math.sqrt()\n",
    "\n",
    "# FILL THIS IN: Calculate the scale factor\n",
    "scale_factor = K.shape[-1]**0.5 # Replace None with your calculation\n",
    "# FILL THIS IN: Divide the raw scores by the scale factor\n",
    "attention_scores_scaled = attention_scores_raw/scale_factor # Replace None with your calculation\n",
    "\n",
    "# ---- CHALLENGE 3 END ----\n",
    "\n",
    "assert scale_factor is not None, \"Challenge 3 incomplete: scale_factor is not calculated.\"\n",
    "assert attention_scores_scaled is not None, \"Challenge 3 incomplete: attention_scores_scaled is not calculated.\"\n",
    "assert attention_scores_scaled.shape == expected_scores_shape, f\"Scaled Attention Scores shape is {attention_scores_scaled.shape}, expected {expected_scores_shape}\"\n",
    "print(f\"Scale factor (sqrt(d_k)): {scale_factor:.2f}\")\n",
    "print(f\"Scaled Attention Scores shape: {attention_scores_scaled.shape} - Correct!\")\n",
    "# print(f\"Scaled Scores (example):\\n{attention_scores_scaled.detach()}\") # Optional: uncomment to view scores\n",
    "print(\"Challenge 3 Completed Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Challenge 4: Apply Softmax ---\n",
      "Attention Weights shape: torch.Size([1, 4, 4]) - Correct!\n",
      "Sum of weights for first query: 1.0000 - Correct!\n",
      "Challenge 4 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 4: Apply Softmax ---\n",
    "# Convert the scaled scores into probability distributions (attention weights).\n",
    "# For each query position (each row in the L x L matrix), the weights across\n",
    "# all key positions (columns) should sum to 1.\n",
    "# This tells us *how much* attention each query should pay to each key.\n",
    "# Formula part: softmax(QK^T / sqrt(d_k))\n",
    "\n",
    "print(\"\\n--- Challenge 4: Apply Softmax ---\")\n",
    "# ---- CHALLENGE 4 START ----\n",
    "# Your task: Apply the softmax function to the `attention_scores_scaled`.\n",
    "# Crucially, apply softmax along the *last* dimension (dim=-1). This ensures\n",
    "# that for each query (row), the weights distributed across the keys (columns) sum to 1.\n",
    "# Hint: Use F.softmax() or torch.softmax()\n",
    "\n",
    "# FILL THIS IN: Apply softmax to the scaled scores along the last dimension\n",
    "attention_weights = torch.softmax(attention_scores_scaled,dim=-1) # Replace None with your calculation\n",
    "\n",
    "# ---- CHALLENGE 4 END ----\n",
    "\n",
    "assert attention_weights is not None, \"Challenge 4 incomplete: attention_weights is not calculated.\"\n",
    "assert attention_weights.shape == expected_scores_shape, f\"Attention Weights shape is {attention_weights.shape}, expected {expected_scores_shape}\"\n",
    "print(f\"Attention Weights shape: {attention_weights.shape} - Correct!\")\n",
    "# Check if weights sum to 1 for the first query position (should be close to 1.0)\n",
    "sum_check = attention_weights[0, 0, :].sum().item()\n",
    "assert math.isclose(sum_check, 1.0, rel_tol=1e-6), f\"Weights for first query sum to {sum_check}, expected ~1.0\"\n",
    "print(f\"Sum of weights for first query: {sum_check:.4f} - Correct!\")\n",
    "# print(f\"Attention Weights (example):\\n{attention_weights.detach()}\") # Optional: uncomment to view weights\n",
    "print(\"Challenge 4 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Challenge 5: Multiply Weights by Values ---\n",
      "Final Output shape: torch.Size([1, 4, 8]) - Correct!\n",
      "Challenge 5 Completed Successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Multiply Weights by Values ---\n",
    "# Now we have the attention weights (how much to focus on each word).\n",
    "# We multiply these weights by the Value (V) vectors.\n",
    "# This effectively creates a weighted sum of the Value vectors, where words\n",
    "# deemed more important (higher attention weight) contribute more to the final output.\n",
    "# Formula part: softmax(QK^T / sqrt(d_k)) * V\n",
    "\n",
    "print(\"\\n--- Challenge 5: Multiply Weights by Values ---\")\n",
    "# ---- CHALLENGE 5 START ----\n",
    "# Your task: Calculate the final output of the self-attention layer.\n",
    "# Perform a matrix multiplication between the `attention_weights` and the Value matrix (V).\n",
    "# attention_weights shape: (N, L, L)\n",
    "# V shape: (N, L, D_v)\n",
    "# The result should be the final context-aware output embeddings, with shape (N, L, D_v).\n",
    "# Hint: Use torch.matmul()\n",
    "\n",
    "# FILL THIS IN: Calculate Weights * V\n",
    "output = attention_weights @ V # Replace None with your calculation\n",
    "\n",
    "# ---- CHALLENGE 5 END ----\n",
    "\n",
    "assert output is not None, \"Challenge 5 incomplete: output is not calculated.\"\n",
    "assert output.shape == expected_v_shape, f\"Final Output shape is {output.shape}, expected {expected_v_shape}\"\n",
    "print(f\"Final Output shape: {output.shape} - Correct!\")\n",
    "print(\"Challenge 5 Completed Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Attention Output ---\n",
      "Output tensor (example first vector):\n",
      "tensor([-0.2372, -0.2045,  0.1046,  0.1264,  0.1372, -0.6009, -0.0637,  0.2800])\n",
      "\n",
      "Congratulations! If you filled everything correctly and saw 'Completed Successfully!' messages,\n",
      "you have implemented a basic self-attention mechanism!\n",
      "This output represents the input sequence where each token's representation\n",
      "has been updated based on its relevance to other tokens in the sequence.\n"
     ]
    }
   ],
   "source": [
    "# --- Final Output ---\n",
    "# The `output` tensor now contains the new representations for each input token.\n",
    "# Each vector in the output sequence (e.g., output[0, i, :]) incorporates information\n",
    "# from the entire input sequence, weighted by the calculated attention scores.\n",
    "# This output can then be passed to subsequent layers in a Transformer model.\n",
    "\n",
    "print(\"\\n--- Attention Output ---\")\n",
    "# We need to make sure output was calculated before trying to print it\n",
    "if output is not None:\n",
    "    print(f\"Output tensor (example first vector):\\n{output[0, 0, :].detach()}\") # Use detach() if not training\n",
    "else:\n",
    "    print(\"Final output not calculated yet.\")\n",
    "\n",
    "print(\"\\nCongratulations! If you filled everything correctly and saw 'Completed Successfully!' messages,\")\n",
    "print(\"you have implemented a basic self-attention mechanism!\")\n",
    "print(\"This output represents the input sequence where each token's representation\")\n",
    "print(\"has been updated based on its relevance to other tokens in the sequence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
